{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "communityname    2215\n",
       "state            2215\n",
       "countyCode        994\n",
       "communityCode     991\n",
       "fold             2215\n",
       "population       2215\n",
       "householdsize    2215\n",
       "racepctblack     2215\n",
       "racePctWhite     2215\n",
       "racePctAsian     2215\n",
       "racePctHisp      2215\n",
       "agePct12t21      2215\n",
       "agePct12t29      2215\n",
       "agePct16t24      2215\n",
       "agePct65up       2215\n",
       "...\n",
       "rapesPerPop            2007\n",
       "robberies              2214\n",
       "robbbPerPop            2214\n",
       "assaults               2202\n",
       "assaultPerPop          2202\n",
       "burglaries             2212\n",
       "burglPerPop            2212\n",
       "larcenies              2212\n",
       "larcPerPop             2212\n",
       "autoTheft              2212\n",
       "autoTheftPerPop        2212\n",
       "arsons                 2124\n",
       "arsonsPerPop           2124\n",
       "ViolentCrimesPerPop    1994\n",
       "nonViolPerPop          2118\n",
       "Length: 147, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.ensemble as sk\n",
    "%pylab inline\n",
    "\n",
    "df = pd.read_csv('./datasets/CommvsCrime.csv')\n",
    "\n",
    "#remove the ':' from the column names\n",
    "columns = list(df.columns)\n",
    "for i, col in enumerate(columns):\n",
    "    columns[i] = col.replace(':', '') \n",
    "df.columns = columns\n",
    "\n",
    "df = df.replace('?', np.nan)  # replace ? with NaN to get a proper count\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 90% of the cities have the ViolentCrimesPerPop or nonViolperPop attribute I will use the sum of these attributes for the measurement of the total crime in a community. I will also be only selecting cities that have both of these attributes. I also will drop the country and community code colums since they aren't very important and are missing a lot of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rows where both values are numbers\n",
    "df.dropna(subset=['ViolentCrimesPerPop', 'nonViolPerPop'], inplace=True)\n",
    "df = df.convert_objects(convert_numeric=True)\n",
    "df['TotalCrime'] = df.nonViolPerPop.add(df.ViolentCrimesPerPop, axis=0)\n",
    "\n",
    "# delete any columns that have fewer rows of numbers\n",
    "for col in df.columns:\n",
    "    if df[col].count() < df.nonViolPerPop.count():\n",
    "     df.drop(col, axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When testing a hypothesis I learned that I should have a train group to train my model and a test group to evaluate how effective my model is. I also realize I don't really have a hypothesis yet, so it looks like I don't know what I am doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "So I guess my hypothesis is that the percentage of minorities in a community is strongly correlated to the ratio between crime and population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# code from stackoverflow to seperate test and training group\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train_data = df[msk]\n",
    "test_data = df[~msk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I've cleaned up my data, I will now start to begin the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now select only the predictors(independent) and the Goals(dependent)\n",
    "predict = train_data.iloc[:,5:104]\n",
    "totalCrime = train_data.TotalCrime\n",
    "\n",
    "# run random Forest\n",
    "rfc = sk.RandomForestClassifier(n_estimators= 1, oob_score=True) # n_estimators originally set at 50\n",
    "model = rfc.fit(predict, totalCrime)\n",
    "\n",
    "print rfc.oob_score_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Mistakes\n",
    "So the out of bag score was 0, and for the longest time I couldn't understand why. It wasn't until I read exactly what the out of bag score was and reviewed Random Forest Classification did I come to understand why my score was 0.0 . The reason is there is no classification in the data. I need to use another form of analysis (like multivariable linear regression) or classify the data by hand, and I've decided to do the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research\n",
    "What I've realized I am interested in is not just the pure crime rate but determining what makes a city have a bad crime rate. So my first order of business is to determine what a bad crime rate is. In the United States there is no formal classification of cities based on their crime rate, this means I will have to create my own definition. The ghetto is one of the worst places to live in the United States due to the fact these areas have high crime rates. Most U.S citizens would agree that a cities crime rate is too high if it is similiar to that of a ghetto. So if I find the average crime rate of ghettos than I can use that information as the basis for my classification.\n",
    "\n",
    "#### Defining Ghetto\n",
    "Turns out the United States does not have any official measurements to determine what constitutes a ghetto, in fact the U.S doesn't even give an official definition of what a ghetto is. Luckily there is a classification officially defined by the U.S Census Bureau called **Concentrated Poverty** that does a good job encapsulating what the ghetto is. The definition given by the bureau is an area with 40 percent of the tract population living below the federal poverty threshold [1]. While it is argued that the 40 percent cutoff is arbitrary, scholars recognize that the cutoff did correspond closely with neighborhoods that city officials and Local Census Bureau Officials considered ghettos [2].\n",
    "\n",
    "So we will now classify each community based on how its crime rate relates to the crime rate of the average area that has concentrated poverty (effectively making it a ghetto).\n",
    "\n",
    "### Getting the data\n",
    "Getting U.S census data was not fun, and was probably a lot more work than it was worth. The data on poverty in the U.S is very granular and goes down all the way to U.S census tracts but the best data I could find for crime in the same time frame is down to the city level for cities with populations above 25,000. In hindsight I probably could've used the crime data from UCI that we are analyzing now but oh well. So I got the poverty levels for the large cities but to make up for the change in size (from U.S census tract to U.S city) I've changed the sample size by selecting areas with poverty levels above 20%. Areas with poverty levels above 20% are still above the national average and are just as concerning since cities are much larger than tracts. \n",
    "\n",
    "After messing around in Excel I was able to clean up the data and get the poverty rate and crime rate (both violent and nonviolent) of 1172 cities across all 50 states. So lets take a look at the csv I made.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "censusdf = pd.read_csv('./datasets/CityCrimePoverty.csv')\n",
    "censusdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The data still has cities with poverty levels below 20%, so I'll filter them out.\n",
    "\n",
    "# The copy() at the end is to prevent the SettingWithCopy warning\n",
    "censusdf = censusdf[(censusdf['Poverty-Individuals'] > 20) | (censusdf['Poverty-Families'] > 20)].copy()\n",
    "censusdf['totalCrime'] = censusdf.Violent.add(censusdf.Nonviolent)\n",
    "censusdf.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The average total crime for \"ghettos\" is 5950 cases per 100,000 people\n",
    "# Now we will classify which communities in our original dataset have crime rates \n",
    "# higher than half of the poverty cities.\n",
    "dangerous = totalCrime > censusdf.totalCrime.quantile(.5)\n",
    "# convert Booleans to integers\n",
    "dangerous = dangerous.astype(int)\n",
    "\n",
    "# Now lets give it another go\n",
    "# run random Forest\n",
    "rfc = sk.RandomForestClassifier(n_estimators= 500, oob_score=True)\n",
    "model = rfc.fit(predict, dangerous)\n",
    "\n",
    "print rfc.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow much better! The performance has improved a lot too, now I can take a look at the importances of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the importances into an array that will be turned into a dataframe\n",
    "fi = enumerate(rfc.feature_importances_)\n",
    "cols = predict.columns\n",
    "features = []\n",
    "for i, value in fi:\n",
    "    features.append([value, cols[i]])\n",
    "\n",
    "featureDF = pd.DataFrame(features, None, ['importance', 'feature'])\n",
    "featureDF.sort('importance', ascending=False, inplace=True)\n",
    "\n",
    "# Lets see the top 10 importances\n",
    "featureDF[0:10]\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features\n",
    "This is interesting it looks like the parenting plays the largest role in determining whether an area is as dangerous as the ghetto 8 out of the top 10 have to do with parenting. This is really insightful, I also thought that having racially segregated areas played the largest role in determining the crime rate but it looks this can be overcome with a strong household.\n",
    "\n",
    "One thing I've also noticed is that I included pure maginitudes in the data which skews things since they aren't adjusted for population differences, this is an area for improvement. \n",
    "\n",
    "Now its time to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the variables from the test data\n",
    "\n",
    "# Independent Variables\n",
    "test_ind = test_data.iloc[:, 5:104]\n",
    "# Dependent Variable\n",
    "test_danger = test_data.TotalCrime > censusdf.totalCrime.quantile(.5)\n",
    "\n",
    "print(\"mean accuracy score for test set = %f\" %(rfc.score(test_ind, test_danger)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Despite all the mistakes made it looks like our model is pretty solid, each time this notebook is run the data is reshuffled and the accuracy of the model is consistantly at 84%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Refrences\n",
    "[1] Kneebone, Elizabeth. \"The Growth and Spread of Concentrated Poverty, 2000 to 2008-2012.\" The Brookings Institution. N.p., 31 July 2014. Web. 25 May 2015.\n",
    "\n",
    "[2] Jargowsky, P. and Bane, M. 1991. “Ghetto Poverty in the United States, 1970 to 1980”. in The Urban Underclass edited by Christopher Jencks and Paul E. Peterson. Washington, D.C.: The Brookings Institution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
